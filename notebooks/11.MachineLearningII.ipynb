{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with `scikit-learn II`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src='https://az712634.vo.msecnd.net/notebooks/python_course/v1/iris-setosa.jpg' alt=\"Smiley face\" width=\"42\" height=\"42\" align=\"left\">Learning Objectives\n",
    "* * *\n",
    "* Become familiar with how supervised learning works in `sklearn`\n",
    "* Become familiar with how unsupervised learning works in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithms - Supervised Learning\n",
    "<img src='https://raw.githubusercontent.com/PythonWorkshop/intro-to-sklearn/master/imgs/ml_process_by_micheleenharris.png' alt=\"Smiley face\" width=\"400\"><br>\n",
    ">  Reminder:  All supervised estimators in scikit-learn implement a `fit(X, y)` method to fit the model and a `predict(X)` method that, given unlabeled observations X, returns the predicted labels y. (direct quote from `sklearn` docs)\n",
    "\n",
    "* Given that Iris is a fairly small, labeled dataset with relatively few features...what algorithm would you start with and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\"\n",
    "\n",
    "> \"Different estimators are better suited for different types of data and different problems.\"\n",
    "\n",
    "<a href = \"http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\" style = \"float: right\">-Choosing the Right Estimator from sklearn docs</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>An estimator for recognizing a new iris from its measurements</b>\n",
    "\n",
    "> Or, in machine learning parlance, we <i>fit</i> an estimator on known samples of the iris measurements to <i>predict</i> the class to which an unseen iris belongs.\n",
    "\n",
    "Let's give it a try!  (We are actually going to hold out a small percentage of the `iris` dataset and check our predictions against the labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will use **`train_test_split`** helper method to split our data into a training and test set.  There will also be another option further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's import sklearn modules\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Let's load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "#   - These namings are used very commonly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try a decision tree classification method\n",
    "from sklearn import tree\n",
    "\n",
    "t = tree.DecisionTreeClassifier(max_depth = 4,\n",
    "                                    criterion = 'entropy', \n",
    "                                    class_weight = 'balanced',\n",
    "                                    random_state = 2) # random_state used for reproducibility\n",
    "t.fit(X_train, y_train)\n",
    "\n",
    "t.score(X_test, y_test) # what performance metric is this, do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Modify the following code in a new cell below.\n",
    "```python\n",
    "# Feed in test data - Fill in blanks\n",
    "y_pred = t.predict(___)\n",
    "\n",
    "# Compare first predicted value to first test value for labels\n",
    "print(\"Prediction: %d, Original label: %d\" % (y_pred[0], ___))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's a nifty way to cross-validate (useful for quick model evaluation!)\n",
    "from sklearn import cross_validation\n",
    "from sklearn import tree\n",
    "\n",
    "t = tree.DecisionTreeClassifier(max_depth = 4,\n",
    "                                    criterion = 'entropy', \n",
    "                                    class_weight = 'balanced',\n",
    "                                    random_state = 2)\n",
    "\n",
    "# Splits, fits and predicts all in one with scoring (does this multiple times on random splits)\n",
    "score = cross_validation.cross_val_score(t, X, y)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "QUESTIONS:  What do these scores tell you?  Are they too high or too low you think?  If it's 1.0, what does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the graph look like for this decision tree?  \n",
    "* i.e. what are the \"questions\" and \"decisions\" for this tree...\n",
    "* Note:  You need both Graphviz app and the python package `graphviz` (It's worth it for this cool decision tree graph, I promise!)\n",
    "* To install both on OS X:\n",
    "```\n",
    "sudo port install graphviz\n",
    "sudo pip install graphviz\n",
    "```\n",
    "* For general Installation see [this guide](http://graphviz.readthedocs.org/en/latest/manual.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's reimport sklearn modules\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Let's load the iris dataset again - what goes here?\n",
    "iris = load_iris()\n",
    "___, ___ = iris.data, iris.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "#   - These namings are used very commonly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# Let's rerun the decision tree classifier\n",
    "from sklearn import tree\n",
    "\n",
    "t = tree.DecisionTreeClassifier(max_depth = 4,\n",
    "                                    criterion = 'entropy', \n",
    "                                    class_weight = 'balanced',\n",
    "                                    random_state = 2)\n",
    "\n",
    "# What method do we use to train our model?\n",
    "t.___(X_train, y_train)\n",
    "\n",
    "t.score(X_test, y_test) # what performance metric is this?\n",
    "\n",
    "export_graphviz(t, out_file=\"mytree.dot\",  \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "\n",
    "with open(\"mytree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "\n",
    "graphviz.Source(dot_graph, format = 'png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Decision Tree to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Modify the code here in a new cell and fill in the blank.\n",
    "\n",
    "```python\n",
    "# Split into training and test (30% into test) - Fill in the blank\n",
    "X_train, X_test, y_train, y_test = ___(X, y, test_size = ___)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Modify the following code in a new cell below.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(max_depth=4,\n",
    "                                criterion = 'entropy', \n",
    "                                n_estimators = 100, \n",
    "                                class_weight = 'balanced',\n",
    "                                n_jobs = -1,\n",
    "                               random_state = 2)\n",
    "# Fill in the blanks below\n",
    "forest.fit(___, y_train)\n",
    "\n",
    "y_preds = iris.target_names[forest.predict(___)]\n",
    "\n",
    "forest.score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use cross validation / scoring method - Fill in the blank\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Reinitialize classifier\n",
    "forest = RandomForestClassifier(max_depth=4,\n",
    "                                criterion = 'entropy', \n",
    "                                n_estimators = 100, \n",
    "                                class_weight = 'balanced',\n",
    "                                n_jobs = -1,\n",
    "                               random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Modify the code below in a new cell to fill in the appropriate method\n",
    "\n",
    "```python\n",
    "# Cross validation / scoring method\n",
    "score = cross_validation.___(forest, X, y)\n",
    "score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION:  Comparing to the decision tree method, what do these accuracy scores tell you?  Do they seem more reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test set vs. cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can be explicit and use the `train_test_split` method in scikit-learn ( [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) ) as in (and as shown above for `iris` data):<p>\n",
    "\n",
    "```python\n",
    "# Create some data by hand and place 70% into a training set and the rest into a test set\n",
    "# Here we are using labeled features (X - feature data, y - labels) in our made-up data\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.70)\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "OR\n",
    "\n",
    "Be more concise and\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, linear_model\n",
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "clf = linear_model.LinearRegression()\n",
    "score = cross_validation.cross_val_score(clf, X, y)\n",
    "```\n",
    "\n",
    "<p>There is also a `cross_val_predict` method to create estimates rather than scores and is very useful for cross-validation to evaluate models ( [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_predict.html) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithms - Unsupervised Learning\n",
    ">  Reminder:  In machine learning, the problem of unsupervised learning is that of trying to find hidden structure in unlabeled data. Since the training set given to the learner is unlabeled, there is no error or reward signal to evaluate a potential solution. Basically, we are just finding a way to represent the data and get as much information from it that we can.\n",
    "\n",
    "HEY!  Remember PCA from above?  PCA is actually considered unsupervised learning in the ML world.  We just put it up there because it's a good way to visualize data at the beginning of the ML process.\n",
    "\n",
    "Let's revisit it in a little more detail using the `iris` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Subset data to have only sepal width (cm) and petal length (cm) for simplification\n",
    "X = iris.data[:, 1:3]\n",
    "print(iris.feature_names[1:3])\n",
    "\n",
    "# Scale it to a gaussian distribution (try it by uncommenting):\n",
    "# from sklearn import preprocessing\n",
    "# X = preprocessing.scale(X)\n",
    "\n",
    "# If n_components is an integer just the top n components that explain\n",
    "#   the most variance are kept (if it's a decimal, it represents percentage of variance explained needed)\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"% of variance attributed to components: \"+ \\\n",
    "      ', '.join(['%.2f' % (x * 100) for x in pca.explained_variance_ratio_]))\n",
    "print('\\ncomponents of each feature:', pca.components_)\n",
    "\n",
    "print(list(zip(pca.explained_variance_, pca.components_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pca.explained_variance_` is like the magnitude of a components influence (amount of variance explained) and the `pca.components_` is like the direction of influence for each feature in each component.\n",
    "\n",
    "<p style=\"text-align:right\"><i>Code in next cell adapted from Jake VanderPlas's code [here](https://github.com/jakevdp/sklearn_pycon2015)</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the original data in X (before PCA)\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "\n",
    "# Grab the component means to get the center point for plot below\n",
    "means = pca.mean_\n",
    "\n",
    "# Here we use the direction of the components in pca.components_\n",
    "#  and the magnitude of the variance explaine by that component in\n",
    "#  pca.explained_variane_\n",
    "\n",
    "# We plot the vector (manginude and direction) of the components\n",
    "#  on top of the original data in X\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.plot([means[0], v[0]+means[0]], [means[1], v[1]+means[1]], '-k', lw=3)\n",
    "\n",
    "\n",
    "# Axis limits\n",
    "plt.xlim(0, max(X[:, 0])+3)\n",
    "plt.ylim(0, max(X[:, 1])+3)\n",
    "\n",
    "# Original feature labels of our data X\n",
    "plt.xlabel(iris.feature_names[1])\n",
    "plt.ylabel(iris.feature_names[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION:  In which direction in the data is the most variance explained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, in the ML I module: unsupervised models have a `fit()`, `transform()` and/or `fit_transform()` in `sklearn`.\n",
    "\n",
    "\n",
    "If you want to both get a fit and new dataset with reduced dimensionality, which would you use below? (Fill in blank in code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Modify this code in a new cell and fill in the blanks.\n",
    "\n",
    "```python\n",
    "# Get back to our 4D dataset\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "pca = PCA(n_components = 0.95) # keep 95% of variance\n",
    "\n",
    "# What method is useful here (to combine two steps...)\n",
    "X_trans = pca.___(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_trans.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot components\n",
    "plt.scatter(X_trans[:, 0], X_trans[:, 1], c = iris.target, edgecolor='none', alpha=0.5,\n",
    "           cmap = plt.cm.get_cmap('spring', 10))\n",
    "plt.ylabel('Component 2')\n",
    "plt.xlabel('Component 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "We will go over the most straightforward of clustering methods, **KMeans**.  KMeans finds cluster centers that are the mean of the points within them.  Likewise, a point is in a cluster because the cluster center is the closest cluster center for that point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you don't have ipywidgets package installed, go ahead and install it now by running the cell below uncommented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "pca = PCA(n_components = 2) # keep 2 components which explain most variance\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I have to tell KMeans how many cluster centers I want\n",
    "n_clusters  = 3\n",
    "\n",
    "# for consistent results when running the methods below\n",
    "random_state = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:right\"><i>Code in next cell adapted from Jake VanderPlas's code [here](https://github.com/jakevdp/sklearn_pycon2015)</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _kmeans_step(frame=0, n_clusters=n_clusters):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    labels = np.zeros(X.shape[0])\n",
    "    centers = rng.randn(n_clusters, 2)\n",
    "\n",
    "    nsteps = frame // 3\n",
    "\n",
    "    for i in range(nsteps + 1):\n",
    "        old_centers = centers\n",
    "        if i < nsteps or frame % 3 > 0:\n",
    "            dist = euclidean_distances(X, centers)\n",
    "            labels = dist.argmin(1)\n",
    "\n",
    "        if i < nsteps or frame % 3 > 1:\n",
    "            centers = np.array([X[labels == j].mean(0)\n",
    "                                for j in range(n_clusters)])\n",
    "            nans = np.isnan(centers)\n",
    "            centers[nans] = old_centers[nans]\n",
    "\n",
    "\n",
    "    # plot the data and cluster centers\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='rainbow',\n",
    "                vmin=0, vmax=n_clusters - 1);\n",
    "    plt.scatter(old_centers[:, 0], old_centers[:, 1], marker='o',\n",
    "                c=np.arange(n_clusters),\n",
    "                s=200, cmap='rainbow')\n",
    "    plt.scatter(old_centers[:, 0], old_centers[:, 1], marker='o',\n",
    "                c='black', s=50)\n",
    "\n",
    "    # plot new centers if third frame\n",
    "    if frame % 3 == 2:\n",
    "        for i in range(n_clusters):\n",
    "            plt.annotate('', centers[i], old_centers[i], \n",
    "                         arrowprops=dict(arrowstyle='->', linewidth=1))\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=np.arange(n_clusters),\n",
    "                    s=200, cmap='rainbow')\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c='black', s=50)\n",
    "\n",
    "    plt.xlim(-4, 5)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.xlabel('PC 1')\n",
    "\n",
    "    if frame % 3 == 1:\n",
    "        plt.text(4.5, 1.7, \"1. Reassign points to nearest centroid\",\n",
    "                 ha='right', va='top', size=8)\n",
    "    elif frame % 3 == 2:\n",
    "        plt.text(4.5, 1.7, \"2. Update centroids to cluster means\",\n",
    "                 ha='right', va='top', size=8)\n",
    "    else:\n",
    "        plt.text(4.5, 1.7, \"3. Updated\",\n",
    "                 ha='right', va='top', size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans employ the <i>Expectation-Maximization</i> algorithm which works as follows: \n",
    "\n",
    "1. Guess cluster centers\n",
    "* Assign points to nearest cluster\n",
    "* Set cluster centers to the mean of points\n",
    "* Repeat 1-3 until converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_clusters, max_clusters = 1, 6\n",
    "interact(_kmeans_step, frame=[0, 20],\n",
    "                    n_clusters=[min_clusters, max_clusters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Warning</b>! There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue.<br>  --Taken directly from sklearn docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty detection aka anomaly detection\n",
    "QUICK QUESTION:\n",
    "What is the diffrence between outlier detection and anomaly detection?\n",
    "\n",
    "Below we will use a one-class support vector machine classifier to decide if a point is anomalous or not given our original data. (The code was adapted from sklearn docs [here](http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#example-svm-plot-oneclass-py) for the `iris` dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import rcParams, font_manager\n",
    "rcParams['figure.figsize'] = (14.0, 7.0)\n",
    "fprop = font_manager.FontProperties(size=14)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "# Iris data (load it) - fill in blank\n",
    "iris = ___()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Taking only two columns...\n",
    "labels = iris.feature_names[1:3]\n",
    "X = X[:, 1:3]\n",
    "\n",
    "# split into training and test sets - fill in blank\n",
    "X_train, X_test, y_train, y_test = ___(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make some outliers\n",
    "X_weird = np.random.uniform(low=-2, high=9, size=(20, 2))\n",
    "\n",
    "# fit the model - fill in blank\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=1, random_state = 0)\n",
    "clf.___(X_train)\n",
    "\n",
    "# predict labels\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_weird)\n",
    "\n",
    "\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about the plotting below.  It's really just so you can visualize the results of your One Class SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up grids for plot and decision boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 9, 500), np.linspace(-2, 9, 500)) # 500x500\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# plot decision boundary and region within\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n",
    "\n",
    "# observation points\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "c = plt.scatter(X_weird[:, 0], X_weird[:, 1], c='red')\n",
    "\n",
    "# helpful info and basic plot settings\n",
    "plt.title(\"Novelty Detection aka Anomaly Detection\")\n",
    "plt.axis('tight')\n",
    "plt.xlim((-2, 9))\n",
    "plt.ylim((-2, 9))\n",
    "plt.ylabel(labels[1], fontsize = 14)\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"best\",\n",
    "           prop=fprop)\n",
    "plt.xlabel(\n",
    "    \"%s\\nerror train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "    \"errors novel abnormal: %d/10\"\n",
    "    % (labels[0], n_error_train, n_error_test, n_error_outliers), fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TRY changing the value of the parameters in the SVM classifier above especially `gamma`.  More information on `gamma` and support vector machine classifiers [here](http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Created by a Microsoft Employee.\n",
    "\t\n",
    "The MIT License (MIT)<br>\n",
    "Copyright (c) 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {
    "81044ebf085c451a81417f99867bea60": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
